{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "import camelot\n",
    "import uuid\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源文件夹\n",
    "PDF_DIRECTORY = \"../data/chatglm_llm_fintech_raw_dataset/allpdf\"\n",
    "# 转化后文本文件夹\n",
    "TXT_DIRECTORY = \"../data/chatglm_llm_fintech_raw_dataset/alltxt\"\n",
    "# 转化后表格文件夹\n",
    "TAB_DIRECTORY = \"../data/chatglm_llm_fintech_raw_dataset/alltable\"\n",
    "# 转化后准备存入向量数据库文件夹\n",
    "VEC_DIRECTORY = \"../data/chatglm_llm_fintech_raw_dataset/alldata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = glob.glob(TAB_DIRECTORY + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11153"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [name for name in file_names if \"txt.txt\" not in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55050666276407657353454500445892807833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid3(uuid.NAMESPACE_OID, \"123\").int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_uuid_dict = {}\n",
    "for file_name in file_names:\n",
    "    basename = os.path.basename(file_name).split('.')[0]\n",
    "    _id = uuid.uuid3(uuid.NAMESPACE_OID, basename).int\n",
    "    name_uuid_dict[basename] = _id\n",
    "\n",
    "with open(\"../data/chatglm_llm_fintech_raw_dataset/uuid.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(name_uuid_dict, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TXT变成JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\'', '\\\"')\n",
    "            line = line.replace('<', '\\\"')\n",
    "            line = line.replace('>', '\\\"')\n",
    "            # 用正则表达式找到所有双引号包围的数组\n",
    "            # matches = re.findall(r'\"\\[.*?\\]\"', line)\n",
    "            matches = re.findall(r'\"\\[.*\\]\"', line)\n",
    "            for match in matches:\n",
    "                # 将数组中的双引号去掉，并替换原字符串中的部分\n",
    "                corrected = match.replace('\\\"', '')\n",
    "                corrected = match[1:-1]\n",
    "                line = line.replace(match, corrected)\n",
    "            try:\n",
    "                row_dict = json.loads(line)\n",
    "            except:\n",
    "                splited_line = line.split(\"\\\"inside\\\": \")\n",
    "                s1, s2 = splited_line[0], splited_line[1]\n",
    "                s2 = s2.replace(\"\\\"\", \"\\'\")\n",
    "                s2 = s2.replace(\"\\\\'\", \"\\\"\")\n",
    "                s2 = s2 if s2[0] != \"\\'\" else \"\\\"\" + s2[1:-2] + \"\\\"}\"\n",
    "                # s2 = s2[:-2] + \"}\" if s2[0] == \"[\" and s2[-3:] == \"\\'}\\n\" else s2\n",
    "                s2 = s2.replace(\"]\\'}\\n\", \"]}\")\n",
    "\n",
    "                formatted = s1+\"\\\"inside\\\": \"+s2\n",
    "                \n",
    "                try:\n",
    "                    row_dict = json.loads(formatted)\n",
    "                except:\n",
    "                    # print(s2[-3:], s2)\n",
    "                    # print(f\"解析错误: {line}\")\n",
    "                    s2 = s2.replace(\"\\\\\", \"\")\n",
    "                    s2 = s2.replace(\"\\\"\", \"\\'\")\n",
    "                    s2 = s2.replace(\"\\n\", \"\")\n",
    "                    s2 = s2.replace(\"}\", \"\")\n",
    "                    s2 = \"\\\"\" + s2 + \"\\\"}\"\n",
    "                    formatted = s1+\"\\\"inside\\\": \"+s2\n",
    "                    try:\n",
    "                        row_dict = json.loads(formatted)\n",
    "                    except:\n",
    "                        print(f\"解析错误: {line}\")\n",
    "                        print(formatted)\n",
    "                    # break\n",
    "            data.append(row_dict)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_title(e):\n",
    "    n = len(e['inside'])\n",
    "    if e['type'] != 'text':\n",
    "        return False\n",
    "    if n > 2 and e['inside'][0] in (\"第\", \"（\", \"(\"):\n",
    "        return True\n",
    "    if n > 1 and e['inside'][0] in \"一二三四五六七八九十\":\n",
    "        return True\n",
    "    if n > 1 and e['inside'][:2] in [f\"{i}{l}\" for i in range(10) for l in ['.', '、']]:\n",
    "        return True\n",
    "    if n > 3 and e['inside'][:3] in [f\"{i}{l}\" for i in range(10, 50) for l in ['.', '、']]:\n",
    "        return True\n",
    "    if n > 2 and e['inside'][:2] in [f\"{i}{l}\" for i in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" for l in ['.', '、']]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToTopic(path, data, level_set, tokenizer):\n",
    "    with open(path, \"w\") as f:\n",
    "        topic_dict = {\"level\":\"\", \"n\":\"\", \"content\":\"\"}\n",
    "        for d in data:\n",
    "            if d['type'] != 'text':\n",
    "                continue\n",
    "            if d['inside'] in level_set:\n",
    "                topic_dict[\"content\"] = topic_dict[\"content\"].replace('\\n', '')\n",
    "                new_text = ''\n",
    "                n = 0\n",
    "                for text in topic_dict[\"content\"]:\n",
    "                    new_text += text\n",
    "                    if text[-1] in ['.', '!', '?', '。', '！', '？', '…', ';', '；', ':', '：', '”', '’', '）', '】', '》', '」',\n",
    "                                    '』', '〕', '〉', '》', '〗', '〞', '〟', '»', '\"', \"'\", ')', ']', '}']:\n",
    "                        input_tokens = tokenizer(new_text)\n",
    "                        new_text = new_text.replace(' ', '')\n",
    "                        if len(input_tokens['input_ids']) > 256:\n",
    "                            # print(\"too long\")\n",
    "                            f.write(new_text[:200])\n",
    "                            f.write('\\n')\n",
    "                            f.write(new_text[200:])\n",
    "                            f.write('\\n')\n",
    "                            new_text = ''\n",
    "                        elif len(input_tokens['input_ids']) > 150:\n",
    "                            f.write(new_text)\n",
    "                            f.write('\\n')\n",
    "                            new_text = ''\n",
    "\n",
    "                topic_dict = {\"level\":\"\", \"content\":\"\"}\n",
    "                topic_dict['level'] = d['inside']\n",
    "            else:\n",
    "                topic_dict[\"content\"] += \" \" + d['inside']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_model = 'D:\\\\code\\\\llm\\\\embeding\\\\text2vec-base-chinese-paraphrase'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, error_list = 0, []\n",
    "for i, file_name in enumerate(file_names):\n",
    "    try:\n",
    "        data = read_data(file_name)\n",
    "        level = [e for e in data if is_title(e)]\n",
    "        level_set = set([e['inside'] for e in level])\n",
    "        path = os.path.join(VEC_DIRECTORY, os.path.basename(file_name))\n",
    "        ToTopic(path, data, level_set, tokenizer)\n",
    "    except:\n",
    "        n += 1\n",
    "        error_list.append(file_name)\n",
    "        print(\"No.{:7d} {} cannot be read\".format(i, os.path.basename(file_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF变成Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = glob.glob(PDF_DIRECTORY + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(file_names[0]).split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_for_table(file_name):\n",
    "    path = os.path.join(TAB_DIRECTORY, os.path.basename(file_names[0]).split('.')[0])\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    tables = camelot.read_pdf(file_name, pages='1-30')\n",
    "    for i, t in enumerate(tables):\n",
    "        with open(os.path.join(path, f\"{i}.json\"), \"w\") as f:\n",
    "            json.dump(t.parsing_report, f)\n",
    "        t.df.to_csv(os.path.join(path, f\"{i}.csv\"), index=False, sep='\\x01')\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列名+是+信息\n",
    "def row_column_data(df):\n",
    "    m, n = df.shape\n",
    "    res_list = []\n",
    "    for i in range(1, m):\n",
    "        for j in range(0, n):\n",
    "            res_list.append(f\"{df.loc[i, 0]}是{df.loc[i, j]}\")\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列名+的+行名+是+信息\n",
    "def column_row_data(df):\n",
    "    m, n = df.shape\n",
    "    res_list = []\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            res_list.append(f\"{df.loc[i, 0]}的{df.loc[0, j]}是{df.loc[i, j]}\")\n",
    "            res_list.append(f\"{df.loc[0, j]}的{df.loc[i, 0]}是{df.loc[i, j]}\")\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToTopic(path, tables):\n",
    "    with open(path, \"w\") as f:\n",
    "        keyword = set()\n",
    "\n",
    "        for t in tables:\n",
    "            df = t.df\n",
    "            df = df.applymap(lambda x: x.replace(\"\\n\", \"\") if type(x) == str else x)\n",
    "\n",
    "            plain_txt_list = []\n",
    "\n",
    "            if df.shape[1] > 2:\n",
    "                plain_txt_list = column_row_data(df)\n",
    "            if df.shape[1] == 2:\n",
    "                plain_txt_list = row_column_data(df)\n",
    "\n",
    "            plain_txt_list = set(plain_txt_list)\n",
    "\n",
    "            for txt in plain_txt_list:\n",
    "                if \"指是指\" in txt:\n",
    "                    continue\n",
    "                if txt[-1] in [\"是\",\"指\"]:\n",
    "                    continue\n",
    "                if txt[0] in [\"的\"]:\n",
    "                    continue\n",
    "                if txt[0].isdigit() and (not txt[1].isdigit()):\n",
    "                    continue\n",
    "                if txt.count(\"%\") >= 2:\n",
    "                    continue\n",
    "                if len(txt) < 2:\n",
    "                    continue\n",
    "                if \"是\" in txt:\n",
    "                    s = txt.split('是')\n",
    "                    if s[0] == s[1]:\n",
    "                        continue\n",
    "                    if s[0] in keyword:\n",
    "                        continue\n",
    "                    keyword.add(s[0])\n",
    "                f.write(txt+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    path = os.path.join(VEC_DIRECTORY, \"table_\"+os.path.basename(file_name))\n",
    "    tables = worker_for_table(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10-117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
